{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8195\n",
      "Number of validation examples: 2049\n",
      "Number of testing examples: 2560\n",
      "\tTrain Loss: 0.836 | Train Acc: 63.51%\n",
      "\t Val. Loss: 0.745 |  Val. Acc: 65.08%\n",
      "\tTrain Loss: 0.724 | Train Acc: 66.84%\n",
      "\t Val. Loss: 0.729 |  Val. Acc: 65.08%\n",
      "\tTrain Loss: 0.704 | Train Acc: 66.84%\n",
      "\t Val. Loss: 0.707 |  Val. Acc: 65.13%\n",
      "\tTrain Loss: 0.673 | Train Acc: 67.58%\n",
      "\t Val. Loss: 0.646 |  Val. Acc: 68.21%\n",
      "\tTrain Loss: 0.625 | Train Acc: 70.37%\n",
      "\t Val. Loss: 0.603 |  Val. Acc: 70.13%\n",
      "\tTrain Loss: 0.591 | Train Acc: 72.01%\n",
      "\t Val. Loss: 0.589 |  Val. Acc: 72.58%\n",
      "\tTrain Loss: 0.568 | Train Acc: 73.69%\n",
      "\t Val. Loss: 0.573 |  Val. Acc: 74.28%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6344898216c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;31m# load weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"saved_weights_LSTM.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;31m# predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from data_preprocessing import get_files, labels_distribution\n",
    "from train_test import create_iterator, run_train, evaluate\n",
    "from lstm_model import LSTM\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # placing the tensors on the GPU if one is available.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "\n",
    "    # parameters\n",
    "    model_type = \"LSTM\"\n",
    "    data_type = \"token\" # or: \"morph\"\n",
    "\n",
    "    char_based = True\n",
    "    if char_based:\n",
    "        tokenizer = lambda s: list(s) # char-based\n",
    "    else:\n",
    "        tokenizer = lambda s: s.split() # word-based\n",
    "\n",
    "\n",
    "    # hyper-parameters:\n",
    "    lr = 1e-4\n",
    "    batch_size = 50\n",
    "    dropout_keep_prob = 0.5\n",
    "    embedding_size = 300\n",
    "    max_document_length = 100  # each sentence has until 100 words\n",
    "    dev_size = 0.8 # split percentage to train\\validation data\n",
    "    max_size = 5000 # maximum vocabulary size\n",
    "    seed = 1\n",
    "    num_classes = 3\n",
    "\n",
    "    # dropout_keep_prob, embedding_size, batch_size, lr, dev_size, vocabulary_size, max_document_length, input_size, hidden_size, output_dim, n_filters, filter_sizes, num_epochs = get_params(model_type)\n",
    "    train_data, valid_data, test_data, Text, Label = get_files(\"\", dev_size, max_document_length, seed, data_type, tokenizer)\n",
    "\n",
    "    # Build_vocab : It will first create a dictionary mapping all the unique words present in the train_data to an\n",
    "    # index and then after it will use word embedding (random, Glove etc.) to map the index to the corresponding word embedding.\n",
    "    Text.build_vocab(train_data, max_size=max_size)\n",
    "    Label.build_vocab(train_data)\n",
    "    vocab_size = len(Text.vocab)\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = create_iterator(train_data, valid_data, test_data, batch_size, device)\n",
    "\n",
    "    # loss function\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    if (model_type == \"LSTM\"):\n",
    "\n",
    "        num_hidden_nodes = 93\n",
    "        hidden_dim2 = 128\n",
    "        num_layers = 2  # LSTM layers\n",
    "        bi_directional = False\n",
    "        num_epochs = 7\n",
    "\n",
    "        to_train = True\n",
    "        pad_index = Text.vocab.stoi[Text.pad_token]\n",
    "\n",
    "        # Build the model\n",
    "        lstm_model = LSTM(vocab_size, embedding_size, num_hidden_nodes, hidden_dim2 , num_classes, num_layers,\n",
    "                       bi_directional, dropout_keep_prob, pad_index)\n",
    "\n",
    "        # optimization algorithm\n",
    "        optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
    "\n",
    "        # train and evaluation\n",
    "        if (to_train):\n",
    "            # train and evaluation\n",
    "            run_train(num_epochs, lstm_model, train_iterator, valid_iterator, optimizer, loss_func, model_type)\n",
    "\n",
    "            # load weights\n",
    "        lstm_model.load_state_dict(torch.load(\"saved_weights_LSTM.pt\"))\n",
    "        # predict\n",
    "        test_loss, test_acc = evaluate(lstm_model, test_iterator, loss_func)\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.Dataset at 0x190db777390>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
